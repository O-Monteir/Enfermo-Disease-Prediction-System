# -*- coding: utf-8 -*-
"""Enfermo - Diabetes prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wA5QO35HDBmA87TQRK24GhIRlS2Yj2lR

Importing Dependencies
"""

import numpy as np
#used for creating numpy arrays
import pandas as pd
#creates data frame
from sklearn.preprocessing import StandardScaler
#used for standardizing data to common range
from sklearn.model_selection import train_test_split
#split data into training and test
from sklearn import svm
#importing support vector machine 
from sklearn.metrics import accuracy_score



"""Data Collection and Analysis

PIMA Diabetes Dataset

"""

#loading the diabetes dataset using pandas data frame

diabetes_dataset= pd.read_csv('/content/diabetes.csv')
#ariable contains dataset

#printing first five rows of dataset
diabetes_dataset.head()
#1 represents patient is diabetic
#0 represents patient is non diabetic

#number of rows and columns in this dataset
diabetes_dataset.shape
#768 people and 9 attributes

#getting statistical measures of the data
diabetes_dataset.describe()

"""
count means number of data points i.e. 786
mean = mean value of each attribute
std = standard deviation
min (25%) means 25% of values are less than 99 (for glucose)
min (25%) means 25% of values are less than 62 (Blood Pressure)
"""

diabetes_dataset['Outcome'].value_counts()
#tells us number of diabetic and non-diabetic patients
#0 is non diabetic
#1 is diabetic

diabetes_dataset.groupby('Outcome').mean()
#the mean for diabetic and non-diabetic
#always group data sets by label

#separating the data and labels
x=diabetes_dataset.drop(columns='Outcome',axis=1)
#axis = 1 if dropping a colum
#axis=0 for dropping row

y=diabetes_dataset['Outcome']

print(x)

print(y)

"""Data Standardization"""

#using stand scaler for data standardization
scaler=StandardScaler()

scaler.fit(x)

standardized_data=scaler.transform(x)
# or use scaler.fit_transform can be used instead of above 2 steps

print(standardized_data)
#all values are in similar range

x=standardized_data
y=diabetes_dataset['Outcome']
#x represents data
#y representd model

print(x)
print(y)

#splitting data into training data and test data
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)

"""
x will be split into 2 arrays train and test
machine learning model cannot see test data
x is data, y is label

train_test_split() gives 4 outputs which will stored as x train x test and y train, y test

x and y represent old data set
test_size represents per centage of data to be used for testing 
0.2 means 20% is used for testing

stratify : preserves the same proportions of examples in each class as observed in the original dataset.

random_state : index for splitting of data
2: split same as dataset
"""

print(x.shape,x_train.shape,x_test.shape)

"""Training the Model"""

classifier = svm.SVC(kernel='linear')
#SVC is support vector classifier
#using a linear model

#training the support vector machine classsifier
classifier.fit(x_train,y_train)
#trained model is stored in classifier variable

"""Model Evaluation

Accuracy Score
"""

#trying to predict label without training data and compare prediction of model to original data

#accuracy score on training data
x_train_prediction = classifier.predict(x_train) #predictions made by model
training_data_accuracy=accuracy_score(x_train_prediction,y_train) #comparison with og model

print('Accuracy score of training data : ',training_data_accuracy)

#accuracy score on test data
x_test_prediction = classifier.predict(x_test)
test_data_accuracy=accuracy_score(x_test_prediction,y_test)

print('Accuracy score of test data : ',test_data_accuracy)

"""Making a predictive system

"""

input_data = (5,166,72,19,175,25.8,0.587,51)

#change input data to numpy array
input_data_as_numpy_array =np.asarray(input_data)

#reshaping the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)
#specifiying we are using only 1 instance

#standardizing input data
std_data=scaler.transform(input_data_reshaped)
print(std_data)

prediction=classifier.predict(std_data)
#standardized data is fed into the machine learning model
print(prediction)
#gives a list value with a single element at index 0

if (prediction[0]==0):
  print("The person is not diabetic")
else:
  print("The person is diabetic")

import pickle

filename='diabetes_model.sav'
pickle.dump(classifier,open(filename,'wb'))

#loading saved model
loaded_model=pickle.load(open('diabetes_model.sav','rb'))

input_data = (5,166,72,19,175,25.8,0.587,51)

#change input data to numpy array
input_data_as_numpy_array =np.asarray(input_data)

#reshaping the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)
#specifiying we are using only 1 instance

#standardizing input data
std_data=scaler.transform(input_data_reshaped)
print(std_data)

prediction=classifier.predict(std_data)
#standardized data is fed into the machine learning model
print(prediction)
#gives a list value with a single element at index 0

if (prediction[0]==0):
  print("The person is not diabetic")
else:
  print("The person is diabetic")

